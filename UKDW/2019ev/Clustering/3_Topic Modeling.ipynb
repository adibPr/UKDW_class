{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third parties module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# local module\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "                                                text                cluster\n",
      "0  I was wondering if anyone out there could enli...              rec.autos\n",
      "1  A fair number of brave souls who upgraded thei...  comp.sys.mac.hardware\n",
      "2  well folks, my mac plus finally gave up the gh...  comp.sys.mac.hardware\n",
      "3  \\nDo you have Weitek's address/phone number?  ...          comp.graphics\n",
      "4  From article <C5owCB.n3p@world.std.com>, by to...              sci.space\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = load_data ('text')\n",
    "# show some data\n",
    "print ('\\n'.join (np.unique (data['cluster'])))\n",
    "print (data.head (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop word and basic preprocessing\n",
    "stopword = nltk.corpus.stopwords.words ('english')\n",
    "punctuation = string.punctuation + '@<?'\n",
    "def preprocessing (text):\n",
    "    # first lowering\n",
    "    text = text.lower ()\n",
    "    # replace punctuation\n",
    "    for punc in punctuation:\n",
    "        text.replace (punc, ' ')\n",
    "    # replace stopwords\n",
    "    words = nltk.word_tokenize (text)\n",
    "    words = [w for w in words if w not in stopword and len (w) >= 3]\n",
    "    \n",
    "    return ' '.join (words)\n",
    "\n",
    "data['token'] = data['text'].apply (preprocessing)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF top 1000 --> normalize\n",
    "vectorizer = TfidfVectorizer (max_features=1000)\n",
    "features = vectorizer.fit_transform (data['token'])\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=15, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 15\n",
    "lda = LatentDirichletAllocation (num_topics)\n",
    "lda.fit (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = lda.transform (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0, centroid : edu soon 800 number answer\n",
      "Topic 1, centroid : card monitor video thanks anyone\n",
      "Topic 2, centroid : space nasa launch test earth\n",
      "Topic 3, centroid : sale 00 offer shipping condition\n",
      "Topic 4, centroid : book books information published reference\n",
      "Topic 5, centroid : deleted stuff ax source sites\n",
      "Topic 6, centroid : drive scsi drives disk controller\n",
      "Topic 7, centroid : god people one would think\n",
      "Topic 8, centroid : game team year games players\n",
      "Topic 9, centroid : windows use file program files\n",
      "Topic 10, centroid : edu david mit university clinton\n",
      "Topic 11, centroid : car one bike like would\n",
      "Topic 12, centroid : edu ftp 1993 pub site\n",
      "Topic 13, centroid : please thanks mail com anyone\n",
      "Topic 14, centroid : would people government one gun\n"
     ]
    }
   ],
   "source": [
    "sorted_topic = lda.components_.argsort ()[:,::-1]\n",
    "terms = vectorizer.get_feature_names ()\n",
    "for k in range (num_topics):\n",
    "    label = []\n",
    "    for t_idx in sorted_topic[k,:5]:\n",
    "        label.append (terms[t_idx])\n",
    "    print (\"Topic {}, centroid : {}\".format (k, ' '.join (label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01014303 0.01014303 0.01014302 0.01014308 0.01014303 0.01014302\n",
      " 0.01014302 0.04163018 0.01014303 0.01014304 0.01014302 0.82651003\n",
      " 0.01014306 0.01014304 0.01014337]\n",
      "\n",
      "1.0\n",
      "\n",
      "14-y-o son usual teenage spotty chin greasy nose bought clearasil face wash ointment think probably enough along usual good diet however get product called dalacin used doctor's-prescription treatment available chemist counter asked couple pharmacists say either acne severe enough dalacin clearasil ok. odd spots teenager nothing serious father n't figure acne going escalate something disfiguring know kids senstitive appearance wary neighbour son wierd malady eventually put overdose vitamin acne treatment want help appropriate treatment son also scaliness around hairline scalp sort teenage cradle cap pointers/advice tried couple anti dandruff shampoos inclined make condition worse better shall bury kid till\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "idx = 31\n",
    "print (mapped_data[idx])\n",
    "print ()\n",
    "print (sum (mapped_data[idx]))\n",
    "print ()\n",
    "print (data.loc[idx, 'token'])\n",
    "print (data.loc[idx, 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
